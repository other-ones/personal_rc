INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.15 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+cu118)
    Python  3.9.18 (you have 3.9.19)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
INFO:__main__:Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'thresholding', 'dynamic_thresholding_ratio', 'timestep_spacing', 'clip_sample_range', 'variance_type', 'sample_max_value', 'prediction_type'} was not found in config. Values will be initialized to default values.
{'scaling_factor'} was not found in config. Values will be initialized to default values.
{'encoder_hid_dim_type', 'class_embed_type', 'num_class_embeds', 'cross_attention_norm', 'addition_embed_type', 'num_attention_heads', 'addition_embed_type_num_heads', 'conv_out_kernel', 'time_embedding_dim', 'projection_class_embeddings_input_dim', 'conv_in_kernel', 'mid_block_type', 'time_cond_proj_dim', 'upcast_attention', 'class_embeddings_concat', 'time_embedding_act_fn', 'addition_time_embed_dim', 'timestep_post_act', 'only_cross_attention', 'resnet_time_scale_shift', 'dual_cross_attention', 'transformer_layers_per_block', 'resnet_skip_time_act', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'use_linear_projection', 'time_embedding_type', 'encoder_hid_dim'} was not found in config. Values will be initialized to default values.
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 10000000
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 2
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 8
INFO:__main__:  Gradient Accumulation steps = 4
INFO:__main__:  Total optimization steps = 1000000
set seed 2940
32 norm_num_groups
320 block_out_channels[0]
32 norm_num_groups
None mask_token_ids
seeded
captions_nonliving_relations	29376
captions_nonliving_styles	378
captions_nonliving_human_interactions	4176
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
seeded
captions_nonliving_relations	29376
captions_nonliving_styles	378
captions_nonliving_human_interactions	4176
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
Steps:   0%|          | 0/1000000 [00:00<?, ?it/s]True accepts_keep_fp32_wrapper
{'keep_fp32_wrapper': True} extra_args
Steps:   0%|          | 0/1000000 [00:13<?, ?it/s, loss=0.192, lr=0.004, norm_target=8.42]Steps:   0%|          | 1/1000000 [00:13<3875:21:02, 13.95s/it, loss=0.192, lr=0.004, norm_target=8.42]Steps:   0%|          | 1/1000000 [00:25<3875:21:02, 13.95s/it, loss=0.396, lr=0.004, norm_target=8.42]Steps:   0%|          | 2/1000000 [00:25<3410:06:40, 12.28s/it, loss=0.396, lr=0.004, norm_target=8.42]Steps:   0%|          | 2/1000000 [00:36<3410:06:40, 12.28s/it, loss=0.0813, lr=0.004, norm_target=8.42]Steps:   0%|          | 3/1000000 [00:36<3262:56:22, 11.75s/it, loss=0.0813, lr=0.004, norm_target=8.42]Steps:   0%|          | 3/1000000 [00:47<3262:56:22, 11.75s/it, loss=0.00564, lr=0.004, norm_target=8.92]Steps:   0%|          | 4/1000000 [00:47<3194:49:00, 11.50s/it, loss=0.00564, lr=0.004, norm_target=8.92]Steps:   0%|          | 4/1000000 [00:58<3194:49:00, 11.50s/it, loss=0.0224, lr=0.004, norm_target=8.92] Steps:   0%|          | 5/1000000 [00:58<3156:34:18, 11.36s/it, loss=0.0224, lr=0.004, norm_target=8.92]Steps:   0%|          | 5/1000000 [01:09<3156:34:18, 11.36s/it, loss=0.11, lr=0.004, norm_target=8.92]  Steps:   0%|          | 6/1000000 [01:09<3133:18:57, 11.28s/it, loss=0.11, lr=0.004, norm_target=8.92]Steps:   0%|          | 6/1000000 [01:20<3133:18:57, 11.28s/it, loss=0.11, lr=0.004, norm_target=8.92]Steps:   0%|          | 7/1000000 [01:20<3119:26:07, 11.23s/it, loss=0.11, lr=0.004, norm_target=8.92]