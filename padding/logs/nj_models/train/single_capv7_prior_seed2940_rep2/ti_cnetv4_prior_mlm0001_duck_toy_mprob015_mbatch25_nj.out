INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.15 (you have 1.4.11). Upgrade using: pip install --upgrade albumentations
WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:
    PyTorch 2.1.1+cu121 with CUDA 1201 (you have 2.1.1+cu118)
    Python  3.9.18 (you have 3.9.19)
  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)
  Memory-efficient attention, SwiGLU, sparse and more won't be available.
  Set XFORMERS_MORE_DETAILS=1 for more details
INFO:__main__:Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

Mixed precision type: no

/home/twkim/anaconda3/envs/context/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
{'clip_sample_range', 'sample_max_value', 'variance_type', 'prediction_type', 'timestep_spacing', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.
{'scaling_factor'} was not found in config. Values will be initialized to default values.
{'encoder_hid_dim', 'time_cond_proj_dim', 'resnet_out_scale_factor', 'resnet_time_scale_shift', 'addition_embed_type', 'upcast_attention', 'use_linear_projection', 'class_embeddings_concat', 'resnet_skip_time_act', 'only_cross_attention', 'dual_cross_attention', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'timestep_post_act', 'mid_block_only_cross_attention', 'time_embedding_act_fn', 'mid_block_type', 'conv_in_kernel', 'transformer_layers_per_block', 'time_embedding_type', 'num_attention_heads', 'time_embedding_dim', 'class_embed_type', 'projection_class_embeddings_input_dim', 'num_class_embeds', 'conv_out_kernel', 'addition_time_embed_dim', 'cross_attention_norm'} was not found in config. Values will be initialized to default values.
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 10000000
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Instantaneous batch size per device = 2
INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 8
INFO:__main__:  Gradient Accumulation steps = 4
INFO:__main__:  Total optimization steps = 1000000
set seed 2940
32 norm_num_groups
320 block_out_channels[0]
32 norm_num_groups
[49409] mask_token_ids
seeded
captions_nonliving_relations	29376
captions_nonliving_styles	378
captions_nonliving_human_interactions	4176
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
seeded
captions_nonliving_relations	29376
captions_nonliving_styles	378
captions_nonliving_human_interactions	4176
captions_nonliving_backgrounds	552
captions_nonliving_creatives	117
position_ids defined_key-clsnet
final.weight defined_key-clsnet
final.bias defined_key-clsnet
position_embedding.weight defined_key-clsnet
multi_head_attention1.in_proj_weight defined_key-clsnet
multi_head_attention1.in_proj_bias defined_key-clsnet
multi_head_attention1.out_proj.weight defined_key-clsnet
multi_head_attention1.out_proj.bias defined_key-clsnet
feed_forward1.W_ff1.weight defined_key-clsnet
feed_forward1.W_ff1.bias defined_key-clsnet
feed_forward1.W_ff2.weight defined_key-clsnet
feed_forward1.W_ff2.bias defined_key-clsnet
layer_norm1.weight defined_key-clsnet
layer_norm1.bias defined_key-clsnet
multi_head_attention2.in_proj_weight defined_key-clsnet
multi_head_attention2.in_proj_bias defined_key-clsnet
multi_head_attention2.out_proj.weight defined_key-clsnet
multi_head_attention2.out_proj.bias defined_key-clsnet
layer_norm2.weight defined_key-clsnet
layer_norm2.bias defined_key-clsnet

module.position_ids saved_key-clsnet
module.final.weight saved_key-clsnet
module.final.bias saved_key-clsnet
module.position_embedding.weight saved_key-clsnet
module.multi_head_attention1.in_proj_weight saved_key-clsnet
module.multi_head_attention1.in_proj_bias saved_key-clsnet
module.multi_head_attention1.out_proj.weight saved_key-clsnet
module.multi_head_attention1.out_proj.bias saved_key-clsnet
module.feed_forward1.W_ff1.weight saved_key-clsnet
module.feed_forward1.W_ff1.bias saved_key-clsnet
module.feed_forward1.W_ff2.weight saved_key-clsnet
module.feed_forward1.W_ff2.bias saved_key-clsnet
module.layer_norm1.weight saved_key-clsnet
module.layer_norm1.bias saved_key-clsnet
module.multi_head_attention2.in_proj_weight saved_key-clsnet
module.multi_head_attention2.in_proj_bias saved_key-clsnet
module.multi_head_attention2.out_proj.weight saved_key-clsnet
module.multi_head_attention2.out_proj.bias saved_key-clsnet
module.layer_norm2.weight saved_key-clsnet
module.layer_norm2.bias saved_key-clsnet
Steps:   0%|          | 0/1000000 [00:00<?, ?it/s]True accepts_keep_fp32_wrapper
{'keep_fp32_wrapper': True} extra_args
Steps:   0%|          | 0/1000000 [00:14<?, ?it/s, loss=0.271, loss_mlm=1.05, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 1/1000000 [00:14<4162:54:30, 14.99s/it, loss=0.271, loss_mlm=1.05, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 1/1000000 [00:26<4162:54:30, 14.99s/it, loss=0.398, loss_mlm=1.16, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 2/1000000 [00:26<3575:55:10, 12.87s/it, loss=0.398, loss_mlm=1.16, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 2/1000000 [00:37<3575:55:10, 12.87s/it, loss=0.0891, loss_mlm=0.892, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 3/1000000 [00:37<3377:16:21, 12.16s/it, loss=0.0891, loss_mlm=0.892, lr=0.004, norm_mask=8.02, norm_target=8.62]Steps:   0%|          | 3/1000000 [00:49<3377:16:21, 12.16s/it, loss=0.00748, loss_mlm=0.788, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 4/1000000 [00:49<3288:04:19, 11.84s/it, loss=0.00748, loss_mlm=0.788, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 4/1000000 [01:00<3288:04:19, 11.84s/it, loss=0.0244, loss_mlm=1.27, lr=0.004, norm_mask=8.02, norm_target=8.94]  Steps:   0%|          | 5/1000000 [01:00<3235:07:46, 11.65s/it, loss=0.0244, loss_mlm=1.27, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 5/1000000 [01:11<3235:07:46, 11.65s/it, loss=0.0977, loss_mlm=0.825, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 6/1000000 [01:11<3202:48:13, 11.53s/it, loss=0.0977, loss_mlm=0.825, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 6/1000000 [01:22<3202:48:13, 11.53s/it, loss=0.169, loss_mlm=0.964, lr=0.004, norm_mask=8.02, norm_target=8.94] Steps:   0%|          | 7/1000000 [01:22<3183:42:25, 11.46s/it, loss=0.169, loss_mlm=0.964, lr=0.004, norm_mask=8.02, norm_target=8.94]Steps:   0%|          | 7/1000000 [01:34<3183:42:25, 11.46s/it, loss=0.0736, loss_mlm=1.02, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 8/1000000 [01:34<3170:39:55, 11.41s/it, loss=0.0736, loss_mlm=1.02, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 8/1000000 [01:45<3170:39:55, 11.41s/it, loss=0.22, loss_mlm=1.17, lr=0.004, norm_mask=8.02, norm_target=9.43]  Steps:   0%|          | 9/1000000 [01:45<3161:30:45, 11.38s/it, loss=0.22, loss_mlm=1.17, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 9/1000000 [01:56<3161:30:45, 11.38s/it, loss=0.12, loss_mlm=0.776, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 10/1000000 [01:56<3155:18:53, 11.36s/it, loss=0.12, loss_mlm=0.776, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 10/1000000 [02:08<3155:18:53, 11.36s/it, loss=0.0343, loss_mlm=1.16, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 11/1000000 [02:08<3150:39:24, 11.34s/it, loss=0.0343, loss_mlm=1.16, lr=0.004, norm_mask=8.02, norm_target=9.43]Steps:   0%|          | 11/1000000 [02:19<3150:39:24, 11.34s/it, loss=0.172, loss_mlm=1.06, lr=0.004, norm_mask=8.02, norm_target=9.92] Steps:   0%|          | 12/1000000 [02:19<3147:56:12, 11.33s/it, loss=0.172, loss_mlm=1.06, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 12/1000000 [02:30<3147:56:12, 11.33s/it, loss=0.0854, loss_mlm=0.926, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 13/1000000 [02:30<3146:24:02, 11.33s/it, loss=0.0854, loss_mlm=0.926, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 13/1000000 [02:42<3146:24:02, 11.33s/it, loss=0.162, loss_mlm=1.24, lr=0.004, norm_mask=8.02, norm_target=9.92]  Steps:   0%|          | 14/1000000 [02:42<3145:54:17, 11.33s/it, loss=0.162, loss_mlm=1.24, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 14/1000000 [02:53<3145:54:17, 11.33s/it, loss=0.036, loss_mlm=0.758, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 15/1000000 [02:53<3143:21:46, 11.32s/it, loss=0.036, loss_mlm=0.758, lr=0.004, norm_mask=8.02, norm_target=9.92]Steps:   0%|          | 15/1000000 [03:04<3143:21:46, 11.32s/it, loss=0.252, loss_mlm=1.2, lr=0.004, norm_mask=8.02, norm_target=10.4]  Steps:   0%|          | 16/1000000 [03:04<3142:20:37, 11.31s/it, loss=0.252, loss_mlm=1.2, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 16/1000000 [03:16<3142:20:37, 11.31s/it, loss=0.0135, loss_mlm=0.793, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 17/1000000 [03:16<3142:03:07, 11.31s/it, loss=0.0135, loss_mlm=0.793, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 17/1000000 [03:27<3142:03:07, 11.31s/it, loss=0.321, loss_mlm=1.09, lr=0.004, norm_mask=8.02, norm_target=10.4]  Steps:   0%|          | 18/1000000 [03:27<3140:37:48, 11.31s/it, loss=0.321, loss_mlm=1.09, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 18/1000000 [03:39<3140:37:48, 11.31s/it, loss=0.0107, loss_mlm=1.21, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 19/1000000 [03:39<3197:29:58, 11.51s/it, loss=0.0107, loss_mlm=1.21, lr=0.004, norm_mask=8.02, norm_target=10.4]Steps:   0%|          | 19/1000000 [03:50<3197:29:58, 11.51s/it, loss=0.00345, loss_mlm=0.751, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 20/1000000 [03:50<3180:23:55, 11.45s/it, loss=0.00345, loss_mlm=0.751, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 20/1000000 [04:01<3180:23:55, 11.45s/it, loss=0.0945, loss_mlm=0.902, lr=0.004, norm_mask=8.02, norm_target=10.9] Steps:   0%|          | 21/1000000 [04:01<3168:23:14, 11.41s/it, loss=0.0945, loss_mlm=0.902, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 21/1000000 [04:13<3168:23:14, 11.41s/it, loss=0.179, loss_mlm=0.845, lr=0.004, norm_mask=8.02, norm_target=10.9] Steps:   0%|          | 22/1000000 [04:13<3160:10:22, 11.38s/it, loss=0.179, loss_mlm=0.845, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 22/1000000 [04:24<3160:10:22, 11.38s/it, loss=0.0844, loss_mlm=0.734, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 23/1000000 [04:24<3153:09:45, 11.35s/it, loss=0.0844, loss_mlm=0.734, lr=0.004, norm_mask=8.02, norm_target=10.9]Steps:   0%|          | 23/1000000 [04:35<3153:09:45, 11.35s/it, loss=0.15, loss_mlm=0.814, lr=0.004, norm_mask=8.02, norm_target=11.4]  Steps:   0%|          | 24/1000000 [04:35<3149:07:39, 11.34s/it, loss=0.15, loss_mlm=0.814, lr=0.004, norm_mask=8.02, norm_target=11.4]Steps:   0%|          | 24/1000000 [04:47<3149:07:39, 11.34s/it, loss=0.0842, loss_mlm=0.754, lr=0.004, norm_mask=8.02, norm_target=11.4]Steps:   0%|          | 25/1000000 [04:47<3145:53:02, 11.33s/it, loss=0.0842, loss_mlm=0.754, lr=0.004, norm_mask=8.02, norm_target=11.4]Steps:   0%|          | 25/1000000 [04:58<3145:53:02, 11.33s/it, loss=0.139, loss_mlm=1.1, lr=0.004, norm_mask=8.02, norm_target=11.4]   Steps:   0%|          | 26/1000000 [04:58<3144:26:35, 11.32s/it, loss=0.139, loss_mlm=1.1, lr=0.004, norm_mask=8.02, norm_target=11.4]Steps:   0%|          | 26/1000000 [05:09<3144:26:35, 11.32s/it, loss=0.0415, loss_mlm=1, lr=0.004, norm_mask=8.02, norm_target=11.4] Steps:   0%|          | 27/1000000 [05:09<3143:18:39, 11.32s/it, loss=0.0415, loss_mlm=1, lr=0.004, norm_mask=8.02, norm_target=11.4]Steps:   0%|          | 27/1000000 [05:21<3143:18:39, 11.32s/it, loss=0.169, loss_mlm=0.588, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 28/1000000 [05:21<3143:35:46, 11.32s/it, loss=0.169, loss_mlm=0.588, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 28/1000000 [05:32<3143:35:46, 11.32s/it, loss=0.152, loss_mlm=0.722, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 29/1000000 [05:32<3142:33:32, 11.31s/it, loss=0.152, loss_mlm=0.722, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 29/1000000 [05:43<3142:33:32, 11.31s/it, loss=0.122, loss_mlm=1.06, lr=0.004, norm_mask=8.02, norm_target=11.8] Steps:   0%|          | 30/1000000 [05:43<3142:05:43, 11.31s/it, loss=0.122, loss_mlm=1.06, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 30/1000000 [05:55<3142:05:43, 11.31s/it, loss=0.216, loss_mlm=1.03, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 31/1000000 [05:55<3142:45:40, 11.31s/it, loss=0.216, loss_mlm=1.03, lr=0.004, norm_mask=8.02, norm_target=11.8]Steps:   0%|          | 31/1000000 [06:06<3142:45:40, 11.31s/it, loss=0.114, loss_mlm=0.603, lr=0.004, norm_mask=8.02, norm_target=12.3]Steps:   0%|          | 32/1000000 [06:06<3142:14:39, 11.31s/it, loss=0.114, loss_mlm=0.603, lr=0.004, norm_mask=8.02, norm_target=12.3]